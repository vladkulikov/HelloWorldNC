{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "enc_system.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladkulikov/HelloWorldNC/blob/main/Code/Notebooks/enc_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6eGPk6quNpM"
      },
      "source": [
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "''' Класс нормировки данных '''\n",
        "class Normalizer():\n",
        "    \n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    dim : int\n",
        "        Размерность входных данных.\n",
        "    range_val_pairs : List[Tuple[float,float]]\n",
        "        Диапазон значений входных данных.\n",
        "    norm_min : float, optional\n",
        "        Нижняя граница нормированных данных (по умолчанию 0).\n",
        "    norm_max : float, optional\n",
        "        Верхняя граница нормированных данных (по умолчанию 1).\n",
        "    '''\n",
        "    def __init__(self, dim : int, range_val_pairs : List[Tuple[float,float]], norm_min : float = 0., norm_max : float = 1.):\n",
        "        self.dim = dim\n",
        "        self.range_val_pairs = range_val_pairs\n",
        "        self.range_val = [(i[1] - i[0]) for i in self.range_val_pairs]\n",
        "        self.norm_min = norm_min\n",
        "        self.norm_max = norm_max\n",
        "        self.range_norm = norm_max - norm_min\n",
        "    \n",
        "    \n",
        "    def __normire(self, entryVal : float, ind : int) -> float:\n",
        "        '''\n",
        "        Parameters\n",
        "        ----------\n",
        "        entryVal : float\n",
        "            Входное значение для нормировки.\n",
        "        ind : int\n",
        "            Индекс элемента в массиве.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            Нормированное значение.\n",
        "\n",
        "        '''\n",
        "        return self.norm_min + ((entryVal - self.range_val_pairs[ind][0]) * self.range_norm / self.range_val[ind])\n",
        "    \n",
        "    def __renormire(self, normVal : float, ind : int) -> float:\n",
        "        '''\n",
        "        Parameters\n",
        "        ----------\n",
        "        normVal : float\n",
        "            Нормированное значение для денормировки.\n",
        "        ind : int\n",
        "            Индекс элемента в массиве.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            Денормированное значение.\n",
        "        '''\n",
        "        return self.range_val_pairs[ind][0] + ((normVal - self.norm_min) / self.range_norm * self.range_val[ind])\n",
        "    \n",
        "    def normalize(self, data) -> List[List[float]]:\n",
        "        '''\n",
        "        Parameters\n",
        "        ----------\n",
        "        data : list, array\n",
        "            Входной набор данных для нормировки.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List[List[float]]\n",
        "            Нормированный набор данных.\n",
        "        '''\n",
        "        count = 0\n",
        "        if type(data) == list:\n",
        "            count = len(data)\n",
        "        else:\n",
        "            count = data.shape[0]\n",
        "        normData = []\n",
        "        for i in range(count):\n",
        "            cur_sample = []\n",
        "            for j in range(self.dim):\n",
        "                cur_sample.append(self.__normire(data[i][j], j))\n",
        "            normData.append(cur_sample)\n",
        "        return normData\n",
        "    \n",
        "    def renormalize(self, normData) -> List[List[float]]:\n",
        "        '''\n",
        "        Parameters\n",
        "        ----------\n",
        "        normData : list, array\n",
        "            Нормированный набор данных.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List[List[float]]\n",
        "            Денормированный набор данных.\n",
        "        '''\n",
        "        count = 0\n",
        "        if type(normData) == list:\n",
        "            count = len(normData)\n",
        "        else:\n",
        "            count = normData.shape[0]\n",
        "        data = []\n",
        "        for i in range(count):\n",
        "            cur_sample = []\n",
        "            for j in range(self.dim):\n",
        "                cur_sample.append(self.__renormire(normData[i][j], j))\n",
        "            data.append(cur_sample)\n",
        "        return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMyXmzKvuVfg"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "#!pip install git+https://github.com/naught101/sobol_seq@v0.2.0#egg=sobol_seq\n",
        "import sobol_seq\n",
        "import time\n",
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "''' Класс генерации данных '''\n",
        "class DataGenerator():\n",
        "\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    dim: int\n",
        "        Размерность входных данных.\n",
        "    val_range : List[Tuple[float,float]]\n",
        "        Диапазон значений входных данных.\n",
        "    '''\n",
        "    def __init__(self, dim : int, val_range : List[Tuple[float,float]]):\n",
        "        try:\n",
        "            assert dim  == len(val_range), 'Размерность входных диапазонов не равна входной размерности!'\n",
        "            self.dim = dim\n",
        "            self.val_range = val_range\n",
        "            random.seed(int(time.time()))\n",
        "        except AssertionError as e:\n",
        "            raise AssertionError(e.args[0])\n",
        "    \n",
        "\n",
        "    def get_random(self, samples_num : int, irrelevant_var_count : int = 0, write_in_file : bool = False) -> List[List[float]]:\n",
        "        '''\n",
        "        Рандомная генерация данных.\n",
        "        Parameters\n",
        "        ----------\n",
        "        samples_num : int\n",
        "            Количество записей.\n",
        "        irrelevant_var_count : int, optional\n",
        "            Количество незначащих переменных. По умолчанию 0.\n",
        "        write_in_file : bool, optional\n",
        "            Запись в файл. По умолчанию False.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List[List[float]]\n",
        "            Список записей.\n",
        "        '''\n",
        "        arr = []\n",
        "        for k in range(samples_num):\n",
        "            sample = []\n",
        "            # добавляем существенные переменные\n",
        "            for i in self.val_range:\n",
        "                sample.append(random.uniform(i[0], i[1]))\n",
        "            # добавляем несущественные переменные\n",
        "            if irrelevant_var_count != 0:\n",
        "                for i in range(irrelevant_var_count):\n",
        "                    sample.append(random.uniform(0., 1.))\n",
        "            arr.append(sample)\n",
        "        if write_in_file:\n",
        "            col = [('x' + str(i+1)) for i in range(self.dim + irrelevant_var_count)]\n",
        "            df = pd.DataFrame(arr, columns=col)\n",
        "            df.to_csv(f'../../DataSet/random_{self.dim}_{samples_num}_{irrelevant_var_count}.csv')\n",
        "        return arr\n",
        "    \n",
        "\n",
        "    def get_sobol(self, samples_num : int, irrelevant_var_count : int = 0, write_in_file : bool = False) -> List[List[float]]:\n",
        "        '''\n",
        "        Генерация данных от 0 до 1 методом Sobol.\n",
        "        Parameters\n",
        "        ----------\n",
        "        samples_num : int\n",
        "            Количество записей.\n",
        "        irrelevant_var_count : int, optional\n",
        "            Количество незначащих переменных. По умолчанию 0.\n",
        "        write_in_file : bool, optional\n",
        "            Запись в файл. По умолчанию False.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List[List[float]]\n",
        "            Список записей.\n",
        "        '''\n",
        "        arr = sobol_seq.i4_sobol_generate(self.dim, samples_num)\n",
        "        if irrelevant_var_count != 0:\n",
        "            zeros = [[0] for i in range(irrelevant_var_count)]\n",
        "            arr = np.insert(arr, obj=self.dim, values=zeros, axis=1)\n",
        "        if write_in_file:\n",
        "            col = [('x' + str(i+1)) for i in range(self.dim + irrelevant_var_count)]\n",
        "            df = pd.DataFrame(arr, columns=col)\n",
        "            df.to_csv(f'../DataSet/sobol_{self.dim}_{samples_num}_{irrelevant_var_count}.csv')\n",
        "        return list(arr)\n",
        "    \n",
        "    \n",
        "    def get_from_file(self, filename : str) -> List[List[float]]:\n",
        "        '''\n",
        "        Parameters\n",
        "        ----------\n",
        "        filename : str\n",
        "            Имя файла.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        OSError\n",
        "            Файл не найден.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List[List[float]]\n",
        "            Список записей.\n",
        "        '''\n",
        "        try:\n",
        "            return list(pd.read_csv(filename, index_col=0).to_numpy('float32'))\n",
        "        except OSError as e:\n",
        "            raise OSError(e.args[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foZwblL7uYOt"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "from keras.layers import Lambda\n",
        "import numpy as np\n",
        "\n",
        "''' Класс автоэнкодеров '''\n",
        "class AutoencoderClass():\n",
        "  def __init__(self, func, input_dim : int, encoding_dim : int, activations : list, enc_type : str, normalizer : Normalizer):\n",
        "    self.func = func                 # Функция обучения\n",
        "    self.batch = 0                   # Размр батча\n",
        "    self.input_dim = input_dim       # Размерность входного представления\n",
        "    self.encoding_dim = encoding_dim # Размерность кодированного представления\n",
        "    self.activations = activations   # Функции активации\n",
        "    self.enc_type = enc_type         # Тип автоэнкодера\n",
        "    self.aes_types = {'dense': self.__create_dense_ae,\n",
        "                      'deep':  self.__create_deep_dense_ae,\n",
        "                      'conv':  self.__create_deep_conv_ae,\n",
        "                      'vae':   self.__create_vae}\n",
        "    self.normalizer = normalizer\n",
        "    try:\n",
        "      # Сборка моделей\n",
        "      self.encoder, self.decoder, self.autoencoder = self.aes_types[self.enc_type]()\n",
        "      if self.aes_types != 'vae':\n",
        "        self.autoencoder.compile(optimizer = 'adam', loss = self.custom_loss, metrics=['accuracy'])\n",
        "      else:\n",
        "        self.autoencoder.compile(optimizer = 'adam', loss = self.vae_loss, metrics=['accuracy'])\n",
        "    except KeyError as e:\n",
        "      raise ValueError('Undefined unit: {}'.format(e.args[0]))\n",
        "\n",
        "  # Обучение модели\n",
        "  def fit(self, train_data, test_data, epochs : int, batch_size : int, shuffle : bool):\n",
        "    self.batch = batch_size\n",
        "    if self.enc_type != 'conv':\n",
        "      self.autoencoder.fit(train_data, train_data,\n",
        "                           epochs=epochs,\n",
        "                           batch_size=self.batch,\n",
        "                           shuffle=shuffle,\n",
        "                           validation_data=(test_data, test_data))\n",
        "    else:\n",
        "      grid_train = []\n",
        "      grid_test = []\n",
        "      for i in range(len(train_data)):\n",
        "        xx, yy = np.meshgrid(train_data[i], train_data[i])\n",
        "        grid_train.append(xx)\n",
        "\n",
        "      for i in range(len(test_data)):\n",
        "        xx, yy = np.meshgrid(test_data[i], test_data[i])\n",
        "        grid_test.append(xx)\n",
        "      \n",
        "      self.autoencoder.fit(grid_train, grid_train,\n",
        "                           epochs=epochs,\n",
        "                           batch_size=self.batch,\n",
        "                           shuffle=shuffle,\n",
        "                           validation_data=(grid_test, grid_test))\n",
        "\n",
        "  # Предсказание результата\n",
        "  def predict(self, x_vector):\n",
        "    if self.enc_type != 'conv':\n",
        "      return self.autoencoder.predict(x_vector)\n",
        "    else:\n",
        "      return self.autoencoder.predict(x_vector)[0]\n",
        "\n",
        "  # Тип автоэнкодера\n",
        "  def get_aec_type(self):\n",
        "    return self.enc_type\n",
        "\n",
        "  # Возвращает собранные модели\n",
        "  def get_models(self):\n",
        "    return self.autoencoder, self.encoder, self.decoder\n",
        "\n",
        "  # Loss функция\n",
        "  @tf.autograph.experimental.do_not_convert\n",
        "  def custom_loss(self, x_true, x_pred):\n",
        "    return K.mean(K.abs(self.func(self.normalizer.renormalize([x_pred])[0]) - self.func(self.normalizer.renormalize([x_true])[0])))\n",
        "\n",
        "  # Loss функция для вариационного автоэнкодера\n",
        "  @tf.autograph.experimental.do_not_convert\n",
        "  def vae_loss(self, x_true, x_pred):\n",
        "    x_true = K.reshape(x_true, shape=(batch_size, self.input_dim))\n",
        "    x_pred = K.reshape(x_pred, shape=(batch_size, self.input_dim))\n",
        "    loss = self.custom_loss(x_true, x_pred)\n",
        "    kl_loss = -0.5 * K.sum(1 + self.z_log_var - K.square(self.z_mean) - K.exp(self.z_log_var))\n",
        "    return loss + kl_loss\n",
        "\n",
        "  ''' Сжимающий автоэнкодер '''\n",
        "  def __create_dense_ae(self):\n",
        "    # Энкодер\n",
        "    input_data = Input(shape=(self.input_dim))\n",
        "    encoded = Dense(self.encoding_dim, activation = self.activations[0])(input_data)\n",
        "    \n",
        "    # Декодер\n",
        "    input_encoded = Input(shape = (self.encoding_dim))\n",
        "    decoded = Dense(self.input_dim, activation = self.activations[1])(input_encoded)\n",
        "\n",
        "    # Модели\n",
        "    encoder = Model(input_data, encoded, name = \"encoder\")\n",
        "    decoder = Model(input_encoded, decoded, name = \"decoder\")\n",
        "    autoencoder = Model(input_data, decoder(encoder(input_data)), name = \"autoencoder\")\n",
        "    return encoder, decoder, autoencoder\n",
        "\n",
        "  ''' Глубокий автоэнкодер '''\n",
        "  def __create_deep_dense_ae(self):\n",
        "    # Энкодер\n",
        "    input_data = Input(shape=(self.input_dim))\n",
        "    x = Dense(self.encoding_dim*2, activation='relu')(input_data)\n",
        "    encoded = Dense(self.encoding_dim, activation='linear')(x)\n",
        "    \n",
        "    # Декодер\n",
        "    input_encoded = Input(shape=(self.encoding_dim,))\n",
        "    x = Dense(self.encoding_dim*2, activation='relu')(input_encoded)\n",
        "    decoded = Dense(self.input_dim, activation='sigmoid')(x)\n",
        "    \n",
        "    # Модели\n",
        "    encoder = Model(input_data, encoded, name=\"encoder\")\n",
        "    decoder = Model(input_encoded, decoded, name=\"decoder\")\n",
        "    autoencoder = Model(input_data, decoder(encoder(input_data)), name=\"autoencoder\")\n",
        "    return encoder, decoder, autoencoder\n",
        "\n",
        "  ''' Сверточный автоэнкодер '''\n",
        "  def __create_deep_conv_ae(self):\n",
        "    # Энкодер\n",
        "    input_data = Input(shape=(self.input_dim, self.input_dim, 1))\n",
        "    x = Conv2D(25, (2, 2), activation='relu', padding='same')(input_data)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    #x = Conv2D(32, (2, 2), activation='relu', padding='same')(x)\n",
        "    #x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    encoded = Conv2D(1, (2, 2), activation='relu', padding='same')(x)\n",
        "\n",
        "    # На этом моменте представление  (7, 7, 1) т.е. 49-размерное\n",
        "    \n",
        "    # Декодер\n",
        "    input_encoded = Input(shape=(7, 7, 1))\n",
        "    #x = Conv2D(32, (7, 7), activation='relu', padding='same')(input_encoded)\n",
        "    #x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(25, (2, 2), activation='relu', padding='same')(input_encoded)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    decoded = Conv2D(1, (2, 2), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    # Модели\n",
        "    encoder = Model(input_data, encoded, name=\"encoder\")\n",
        "    decoder = Model(input_encoded, decoded, name=\"decoder\")\n",
        "    autoencoder = Model(input_data, decoder(encoder(input_data)), name=\"autoencoder\")\n",
        "    return encoder, decoder, autoencoder\n",
        "\n",
        "  ''' Вариационный автоэнкодер                         '''\n",
        "  ''' Работает на основе девергенции Кульбака-Лейблера '''\n",
        "  ''' Идея: переход данных скрытого слоя к нормальному распределению'''\n",
        "  ''' Статья: https://habr.com/ru/post/484756/ '''\n",
        "  ''' Видео:  https://youtu.be/ebI3JLAcWqQ '''\n",
        "  def __create_vae(self):\n",
        "    hidden_dim = 2\n",
        "\n",
        "    input_data = Input(shape=(self.input_dim))\n",
        "    x = Dense(self.encoding_dim, activation='relu')(input_data)\n",
        "    \n",
        "    self.z_mean = Dense(self.encoding_dim)(x)    # Мат ожидание\n",
        "    self.z_log_var = Dense(self.encoding_dim)(x) # Логарифм дисперсии\n",
        "    \n",
        "    # Нормальное распределение N(0, 1)\n",
        "    def noiser(args):\n",
        "      self.z_mean, self.z_log_var = args\n",
        "      N = K.random_normal(shape=(self.batch, self.encoding_dim), mean=0., stddev=1.0)\n",
        "      return K.exp(self.z_log_var / 2) * N + self.z_mean\n",
        "    \n",
        "    # Преобразование данных в нормальное распределения\n",
        "    h = Lambda(noiser, output_shape=(self.encoding_dim,))([self.z_mean, self.z_log_var])\n",
        "    \n",
        "    input_encoded = Input(shape=(self.encoding_dim,))\n",
        "    d = Dense(self.encoding_dim, activation='relu')(input_encoded)\n",
        "    decoded = Dense(self.input_dim, activation='sigmoid')(d)\n",
        "    \n",
        "    encoder = Model(input_data, h, name='encoder')\n",
        "    decoder = Model(input_encoded, decoded, name='decoder')\n",
        "    vae = Model(input_data, decoder(encoder(input_data)), name=\"vae\")\n",
        "    return encoder, decoder, vae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJFa6Av_hMvs"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def func(x):\n",
        "  return x[0]*x[1] + x[1]*x[2] + x[2]*x[3] + x[3]*x[4] + x[4]*x[5] + x[5]*x[5] + x[6]*x[6] + x[7]*x[1]\n",
        "\n",
        "def disp_res(orig_x, orig_y, pred_x, pred_y):\n",
        "  n = len(orig_x)\n",
        "  for i in range(n):\n",
        "    print(f'Orig X: {orig_x[i]}')\n",
        "    print(f'Orig Y: {orig_y[i]}')\n",
        "    print(f'Pred X: {pred_x[i]}')\n",
        "    print(f'Pred X: {pred_y[i]}\\n')\n",
        "\n",
        "\n",
        "def compare(orig_data, pred_data):\n",
        "  # clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
        "  # scores_x = cross_val_score(clf, orig_data[0:10], pred_data[0:10], cv=5)\n",
        "  # scores_y = cross_val_score(clf, [func(x) for x in orig_data][0:10], [func(x) for x in pred_data][0:10], cv=5)\n",
        "  y_orig = [func(x) for x in orig_data]\n",
        "  y_pred = [func(x) for x in pred_data]\n",
        "  k = 10\n",
        "  disp_res(orig_data[0:k], y_orig[0:k], pred_data[0:k], y_pred[0:k])\n",
        "\n",
        "  x_error = mean_absolute_error(orig_data, pred_data)\n",
        "  y_error = mean_absolute_error(y_orig, y_pred)\n",
        "\n",
        "  print(f'Abs error X: {x_error:.2f}')\n",
        "  print(f'Abs error Y: {y_error:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DtsiTontXlf"
      },
      "source": [
        " #данные, тип автоэнкодера, шаг эпох, шаг батчей, шаг размерности, исходная размерность, размерность незначимых, шаг разбиения на выборку, количество точек\n",
        "import sys\n",
        "from numpy import arange\n",
        "def searchOpt(sobol_data ,enc_type : str, h_epoch : int, h_batch : int, h_size : int, dim : int, irr_dim : int, h_percent : float, n : int):\n",
        "  error = sys.float_info.max\n",
        "  hp_list = list()\n",
        "  for percent in arange(0.5, 1.0, h_percent):\n",
        "    for size in range(dim // 2, dim ,h_size):\n",
        "      for batch in range(16, 256, h_batch):\n",
        "        for epoch in range(5, 60, h_epoch):\n",
        "            data_train = np.array(sobol_data[0:int(n * h_percent)])\n",
        "            data_test = np.array(sobol_data[int(n * h_percent):n])\n",
        "            model = AutoencoderClass(func, dim + irr_dim, size, list(['relu', 'sigmoid']), enc_type, normalizer)\n",
        "            model.fit(data_train, data_test, epoch, batch, True)\n",
        "            rand_data = generator.get_random(100)\n",
        "            pred_data = normalizer.renormalize([model.predict(np.array(x).reshape(1,dim + irr_dim))[0] for x in normalizer.normalize(rand_data)])\n",
        "            if error > compare(rand_data, pred_data):\n",
        "              error = compare(rand_data, pred_data)\n",
        "              hp_list.clear()\n",
        "              hp_list.append(enc_type)\n",
        "              hp_list.append(epoch)\n",
        "              hp_list.append(batch)\n",
        "              hp_list.append(size)\n",
        "              hp_list.append(percent)\n",
        "  return hp_list"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9ndsDOjUubS-",
        "outputId": "bef1e59e-bcfa-4e91-88e5-f6d2602a64a2"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  dim = 8\n",
        "  irr_dim = 0\n",
        "  data_range = [(0, 100), (0, 100), (0, 100), (0, 100), (0, 100), (0, 100), (0, 100), (0, 100)]\n",
        "  generator = DataGenerator(dim, data_range)\n",
        "  normalizer = Normalizer(dim, data_range)\n",
        "\n",
        "  n = 150000\n",
        "  sobol_data = generator.get_sobol(n, irr_dim)\n",
        "  random.shuffle(sobol_data)\n",
        "  searchOpt(sobol_data, 'vae', 2, 16, 1, dim, irr_dim, 0.1, n)\n",
        "  data_train = np.array(sobol_data[0:int(n * 0.8)])\n",
        "  data_test = np.array(sobol_data[int(n * 0.8):n])\n",
        "\n",
        "  model = AutoencoderClass(func, dim + irr_dim, 5, list(['relu', 'sigmoid']), 'vae', normalizer)\n",
        "  model.fit(data_train, data_test, 15, 50, True)\n",
        "\n",
        "  rand_data = generator.get_random(100)\n",
        "  pred_data = normalizer.renormalize([model.predict(np.array(x).reshape(1,dim + irr_dim))[0] for x in normalizer.normalize(rand_data)])\n",
        "  #pred_data = normalizer.renormalize([model.predict(x.reshape(1,dim + irr_dim))[0] for x in sobol_data[0:100]])\n",
        "  #compare(normalizer.renormalize(sobol_data)[0:100], pred_data[0:100])\n",
        "  \n",
        "  compare(rand_data, pred_data)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "935/938 [============================>.] - ETA: 0s - loss: 7708.7900 - accuracy: 0.1316"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-818bcf2e7d2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0msobol_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sobol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mirr_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msobol_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0msearchOpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msobol_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vae'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mirr_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msobol_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msobol_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-e4779173f3da>\u001b[0m in \u001b[0;36msearchOpt\u001b[0;34m(sobol_data, enc_type, h_epoch, h_batch, h_size, dim, irr_dim, h_percent, n)\u001b[0m\n\u001b[1;32m     12\u001b[0m            \u001b[0mdata_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msobol_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh_percent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m            \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoencoderClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mirr_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m            \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m            \u001b[0mrand_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m            \u001b[0mpred_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mirr_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-5b0861f2e6d1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, test_data, epochs, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     40\u001b[0m                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                            \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                            validation_data=(test_data, test_data))\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mgrid_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  Incompatible shapes: [8,4] vs. [16,4]\n\t [[node vae/encoder/lambda_9/mul\n (defined at <ipython-input-11-5b0861f2e6d1>:167)\n]] [Op:__inference_train_function_22888]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node vae/encoder/lambda_9/mul:\nIn[0] vae/encoder/lambda_9/Exp (defined at /usr/local/lib/python3.7/dist-packages/keras/backend.py:2711)\t\nIn[1] vae/encoder/lambda_9/random_normal (defined at /usr/local/lib/python3.7/dist-packages/keras/backend.py:6181)\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n>>>     handler_func(fileobj, events)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n>>>     self._handle_recv()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n>>>     self._run_callback(callback, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n>>>     callback(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n>>>     return self.dispatch_shell(stream, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n>>>     handler(stream, idents, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n>>>     user_expressions, allow_stdin)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n>>>     if self.run_code(code, result):\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-43-818bcf2e7d2a>\", line 11, in <module>\n>>>     searchOpt(sobol_data, 'vae', 2, 16, 1, dim, irr_dim, 0.1, n)\n>>> \n>>>   File \"<ipython-input-41-e4779173f3da>\", line 14, in searchOpt\n>>>     model.fit(data_train, data_test, epoch, batch, True)\n>>> \n>>>   File \"<ipython-input-11-5b0861f2e6d1>\", line 42, in fit\n>>>     validation_data=(test_data, test_data))\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 452, in call\n>>>     inputs, training=training, mask=mask)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 452, in call\n>>>     inputs, training=training, mask=mask)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/layers/core/lambda_layer.py\", line 196, in call\n>>>     result = self.function(inputs, **kwargs)\n>>> \n>>>   File \"<ipython-input-11-5b0861f2e6d1>\", line 167, in noiser\n>>>     return K.exp(self.z_log_var / 2) * N + self.z_mean\n>>> "
          ]
        }
      ]
    }
  ]
}